<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Learning from data: Error propagation and nuisance parameters">

<title>Learning from data: Error propagation and nuisance parameters</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 1,
 'sections': [('Why Bayes is Better', 1, None, '___sec0'),
              ('Quotes from one pioneering and one renaissance Bayesian '
               'authority',
               3,
               None,
               '___sec1'),
              ('Advantages of the Bayesian approach', 2, None, '___sec2'),
              ("Occam's razor", 3, None, '___sec3'),
              ('Nuisance parameters (I)', 2, None, '___sec4'),
              ('Nuisance parameters (II): marginal distributions',
               2,
               None,
               '___sec5'),
              ('Error propagation (I): marginalization', 2, None, '___sec6'),
              ('Error propagation (II): changing variables and prior '
               'information',
               2,
               None,
               '___sec7'),
              ('A useful short cut', 3, None, '___sec8'),
              ('Example: Taking the square root of a number',
               3,
               None,
               '___sec9')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="ErrorPropagation-bs.html">Learning from data: Error propagation and nuisance parameters</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="#___sec0" style="font-size: 80%;"><b>Why Bayes is Better</b></a></li>
     <!-- navigation toc: --> <li><a href="#___sec1" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Quotes from one pioneering and one renaissance Bayesian authority</a></li>
     <!-- navigation toc: --> <li><a href="#___sec2" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Advantages of the Bayesian approach</a></li>
     <!-- navigation toc: --> <li><a href="#___sec3" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Occam's razor</a></li>
     <!-- navigation toc: --> <li><a href="#___sec4" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Nuisance parameters (I)</a></li>
     <!-- navigation toc: --> <li><a href="#___sec5" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Nuisance parameters (II): marginal distributions</a></li>
     <!-- navigation toc: --> <li><a href="#___sec6" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Error propagation (I): marginalization</a></li>
     <!-- navigation toc: --> <li><a href="#___sec7" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Error propagation (II): changing variables and prior information</a></li>
     <!-- navigation toc: --> <li><a href="#___sec8" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A useful short cut</a></li>
     <!-- navigation toc: --> <li><a href="#___sec9" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Example: Taking the square root of a number</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<!-- ------------------- main content ---------------------- -->



<div class="jumbotron">
<center><h1>Learning from data: Error propagation and nuisance parameters</h1></center>  <!-- document title -->

<p>
<!-- author(s): Christian Forss&#233;n -->

<center>
<b>Christian Forss&#233;n</b> 
</center>

<p>
<!-- institution -->

<center><b>Department of Physics, Chalmers University of Technology, Sweden</b></center>
<br>
<p>
<center><h4>Sep 24, 2019</h4></center> <!-- date -->
<br>
<p>
<!-- potential-jumbotron-button -->
</div> <!-- end jumbotron -->

<!-- !split -->

<h1 id="___sec0" class="anchor">Why Bayes is Better </h1>

<h3 id="___sec1" class="anchor">Quotes from one pioneering and one renaissance Bayesian authority </h3>

Laplace:
<blockquote>
    <em>"Probability theory is nothing but common sense reduced to calculation."</em>
</blockquote>


<p>
Sivia
<blockquote>
    <em>"Bayesian inference probabilities are a measure of our state of knowledge about nature, not a measure of nature itself."</em>
</blockquote>


<p>
<!-- !split -->

<h2 id="___sec2" class="anchor">Advantages of the Bayesian approach </h2>
<!-- !bpop -->

<ol>
<li> Provides an elegantly simple and rational approach for answering, in an optimal way, any scientific question for a given state of information. This contrasts to the recipe or cookbook approach of conventional statistical analysis. The procedure is well-defined:</li>

<ul>
  <li> Clearly state your question and prior information.</li>
  <li> Apply the sum and product rules. The starting point is always Bayes&#8217; theorem.</li>
</ul>

<li> For some problems, a Bayesian analysis may simply lead to a familiar statistic. Even in this situation it often provides a powerful new insight concerning the interpretation of the statistic.</li>
<li> Incorporates relevant prior (e.g., known signal model or known theory model expansion) information through Bayes&#8217; theorem. This is one of the great strengths of Bayesian analysis.</li>

<ul>
  <li> For data with a small signal-to-noise ratio, a Bayesian analysis can frequently yield many orders of magnitude improvement in model parameter estimation, through the incorporation of relevant prior information about the signal model.</li>
</ul>

<li> Provides a way of eliminating nuisance parameters through marginalization. For some problems, the marginalization can be performed analytically, permitting certain calculations to become computationally tractable.</li>
<li> Provides a way for incorporating the effects of systematic errors arising from both the measurement operation and theoretical model predictions.</li>
<li> Calculates probability of hypothesis directly: \( p(H_i|D, I) \).</li>
<li> Provides a more powerful way of assessing competing theories at the forefront of science by automatically quantifying Occam&#8217;s razor.</li>
</ol>

<!-- !epop -->

<p>
<!-- !split -->
The Bayesian quantitative Occam&#8217;s razor can also save a lot of time that might otherwise be spent chasing noise artifacts that masquerade as possible detections of real phenomena.

<p>
<!-- !split -->

<h3 id="___sec3" class="anchor">Occam's razor </h3>

Occam&#8217;s razor is a principle attributed to the medieval philosopher William of Occam (or Ockham). The principle states that one should not make more assumptions than the minimum needed. It underlies all scientific modeling and theory building. It cautions us to choose from a set of otherwise equivalent models of a given phenomenon the simplest one. In any given model, Occam&#8217;s razor helps us to "shave off" those variables that are not really needed to explain the phenomenon. It was previously thought to be only a qualitative principle.

<p>
<!-- !split -->

<p>
<center>  <!-- FIGURE -->
<hr class="figure">
<center><p class="caption">Figure 1:  Did the Leprechaun drink your wine, or is there a simpler explanation? </p></center>
<p><img src="fig/Leprechaun_or_Clurichaun.png" align="bottom" width=500></p>
</center>

<p>
<!-- !split -->

<h2 id="___sec4" class="anchor">Nuisance parameters (I) </h2>
See demonstration notebook: A Bayesian Billiard game

<p>
<!-- !split -->

<h2 id="___sec5" class="anchor">Nuisance parameters (II): marginal distributions </h2>
Assume that we have a model with two parameters, \( \theta0,\theta_1 \), although only one of them (say \( \theta_1 \)) is of physical relevance (the other one is a nuisance parameter). Through a Bayesian data analysis we have the joint, posterior pdf
$$
p(\theta_0, \theta_1 | D, I).
$$

The marginal posterior pdf \( p(\theta_1 | D, I) \) is obtained via marginalization
$$
p(\theta_1 | D, I) = \int p(\theta_0, \theta_1 | D, I) d\theta_0.
$$

Assume that we have \( N \) samples from the joint pdf. This might be the Markov Chain from an MCMC sampler: \( \left\{ (\theta_0, \theta_1)_i \right\}_{i=0}^{N-1} \). Then the marginal distribution of \( \theta_1 \) will be given by the same chain by simply ignoring the \( \theta_0 \) column, i.e., \( \left\{ \theta_{1,i} \right\}_{i=0}^{N-1} \).

<p>
See the interactive demos created by Chi Feng for an illustration of this: <a href="https://chi-feng.github.io/mcmc-demo/" target="_self">The Markov-chain Monte Carlo Interactive Gallery</a>.

<p>
<!-- !split -->

<h2 id="___sec6" class="anchor">Error propagation (I): marginalization </h2>
The Bayesian approach offers a straight-forward approach for dealing with (known) systematic uncertainties; namely by marginalization. Let us demonstrate this with an example <br />

<p>
<!-- !split -->
<b>Inferring galactic distances with an imprecise knowledge of the Hubble constant</b>
The Hubble constant acts as a galactic ruler as it is used to measure astronomical distances according to \( v = H_0 x \). An error in this ruler will therefore correspond to a systematic uncertainty in such measurements.

<p>
Here we use marginalization to obtain the desired posterior pdf \( p(x|D,I) \) from the joint distribution of \( p(x,H_0|D,I) \)
$$
\begin{equation}
p(x|D,I) = \int_{-\infty}^\infty dH_0 p(x,H_0|D,I).
\label{eq:marginalization}
\end{equation}
$$

<p>
Using Bayes' rule: \( p(x,H_0|D,I) \propto p(D|x,H_0,I) p(x,H_0|I) \), the product rule: \( p(x,H_0|I) = p(H_0|x,I)p(x|I) \), and the fact that \( H_0 \) is independent of \( x \): \( p(H_0|x,I) = p(H_0|I) \), we find that
$$
p(x|D,I) \propto p(x|I) \int dH_0 p(H_0|I) p(D|x,H_0,I),
$$

which means that we have expressed the quantity that we want (the posterior of \( x \)) in terms of quantities that we know.

<p>
Assume that the pdf \( p(H_0 | I) \) is known via its \( N \) samples \( \{H_{i}\}_{i=0}^{N-1} \) generated by the MCMC sampler.

<p>
This means that we can approximate 
$$
p(x |D,I) \propto \int dH_0 p(H_0|I) p(D|x,H_0,I) \approx \frac{1}{N} \sum_{i=1}^N p(D | x, H_i, I)
$$

where we have used a uniform prior for the distance \( p(x|I) \propto 1 \).

<p>
<!-- !split -->

<h2 id="___sec7" class="anchor">Error propagation (II): changing variables and prior information </h2>
(Based on Sivia, ch 3.6.)

<p>
Assume that we have measured parameter \( X = 10 \pm 3 \) and \( Y=7 \pm 2 \); what can we say about the difference \( X-Y \) or the raio \( X/Y \), or the sum of their squares \( X^2+Y^2 \), etc? In essence, the problem is nothing more than an exercise in the change of variables: given the joint pdf \( p(X,Y|I) \), where the information \( I \) might include the data if the pdf is a posterior from a data analysis, we need the corresponding pdf \( p(Z|I) \), where \( Z=X-Y \), or \( Z=X/Y \), or whatever as appropriate.

<p>
<!-- !split -->
Let us start with a single variable \( X \) and a function \( Y=f(X) \). How is \( p(X|I) \) related to \( p(Y|I) \)?

<p>
Consider a point \( X^* \) and a small interval \( \delta X \) around it. The probability that \( X \) lies within that interval can be written
$$
p \left( X^* - \frac{\delta X}{2} \le X < X^* + \frac{\delta X}{2} \big| I \right) 
\approx p(X=X^*|I) \delta X.
$$

<p>
<!-- !split -->
Assume now that the function \( f \) will map the point \( X=X^* \) uniquely onto \( Y=Y^*=f(X^*) \). Then there must be an interval \( \delta Y \) around \( Y^* \) so that the probability is conserved
$$
p(X=X^*|I) \delta X = p(Y=Y^*|I) \delta Y.
$$

<p>
<!-- !split -->
In the limit of infinitesimally small intervals, and with the realization that this should be true for any point \( X \), we obtain the relationship
$$
\begin{equation}
p(X|I) = p(Y=Y|I) \left| \frac{dY}{dX} \right|,
\label{eq:transformation}
\end{equation}
$$

where the term on the far right is called the <em>Jacobian</em>.

<p>
<!-- !split -->
The generalization to several variables, relating the pdf for \( M \) variables \( \{ X_j \} \) in terms of the same number of quantities \( \{ Y_j \} \) related to them, is
$$
\begin{equation}
p(\{X_j\}|I) = p(\{Y_j\}|I) \left| \frac{\partial (Y_1, Y_2, \ldots, Y_M)}{\partial (X_1, X_2, \ldots, X_M)} \right|,
\label{eq:multivariate-transformation}
\end{equation}
$$

where the multivariate Jacobian is given by the determinant of the \( M \times M \) matrix of partial derivatives \( \partial Y_i / \partial X_j \).

<p>
<!-- !split -->
<div class="panel panel-danger">
  <div class="panel-heading">
  <h3 class="panel-title">Summary</h3>
  </div>
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
We have now seen the basic ingredients required for the propagation of errors: it either involves a transformation in the sense of Eq. \eqref{eq:multivariate-transformation} or an integration as in Eq. \eqref{eq:marginalization}.
</div>
</div>


<p>
<!-- !split -->

<h3 id="___sec8" class="anchor">A useful short cut </h3>

For practical purposes, we are often satisfied to approximate pdfs with Gaussians. Within such limits there is an easier method that is often used for error propagation. Note, however, that there are instances when this method fails miserably as will be shown in the example further down.

<p>
Suppose that we have summarized the pdfs \( p(X|I) \) and \( p(Y|I) \) as two Gaussians with mean and standard deviation \( x_0, \sigma_x \) and \( y_0, \sigma_y \), respectively. Assume further that these two variables are not correlated, i.e., \( p(X,Y|I) = p(X|I) p(Y|I) \).

<p>
<!-- !split -->
Suppose now that we are interested in \( Z=X-Y \). Intuitively, we might guess that the best estimate \( z_0 = x_0 - y_0 \), but the error bar \( \sigma_z \) requires some more thought. Differentiate the relation
$$
\delta Z = \delta X - \delta Y.
$$

Square both sides and integrate to get the expectation value
$$
\langle \delta Z^2 \rangle = \langle \delta X^2 + \delta Y^2 - 2 \delta x \delta Y \rangle = \langle \delta X^2 \rangle + \langle  \delta Y^2 \rangle - 2 \langle \delta X \delta Y \rangle,
$$

where we have employed the linear property for an integral over a sum of terms.

<p>
<!-- !split -->
Since we assumed that the pdfs for \( X \) and \( Y \) were described by independent Gaussians we have
$$
\begin{equation}
\langle \delta X^2 \rangle = \sigma_x^2; \qquad \langle \delta Y^2 \rangle = \sigma_y^2; \qquad \langle \delta X \delta Y \rangle = 0,
\label{eq:stddev}
\end{equation}
$$

and we find that
$$
\sigma_z = \sqrt{ \langle \delta Z^2 \rangle } = \sqrt{ \sigma_x^2 + \sigma_y^2 }.
$$

<p>
<!-- !split -->
Consider, as a second example, the ratio of two parameters \( Z = X/Y \). Differentiation gives
$$
\delta Z = \frac{Y \delta X - X \delta Y}{Y^2} \quad \Leftrightarrow \quad \frac{\delta Z}{Z} = \frac{\delta X}{X} - \frac{\delta Y}{Y}.
$$

<p>
Squaring both sides and taking the expectation values, we obtain
$$
\frac{\langle \delta Z^2 \rangle}{z_0^2} = \frac{\langle \delta X^2 \rangle}{x_0^2} + \frac{\langle \delta Y^2 \rangle}{y_0^2} - 2 \frac{\langle \delta X \rangle \langle \delta ZY \rangle}{x_0 y_0},
$$

where the \( X \), \( Y \) and \( Z \) in the denominator have been replaced by the constants \( x_0 \), \( y_0 \) and \( z_0 = x_0 / y_0 \) because we are interested in deviations from the peak of the pdf.

<p>
<!-- !split -->
Finally, substituting the information for the pdfs of \( X \) and \( Y \) as summarized in Eq. \eqref{eq:stddev} we finally obtain the propagated error for the ratio
$$
\frac{\sigma_z}{z_0} = \sqrt{ \left( \frac{\sigma_x}{x_0} \right)^2 + \left( \frac{\sigma_y}{y_0} \right)^2}.
$$

<p>
<!-- !split -->
Despite its virtues, let us end our discussion of error-propagation with a salutary warning against the blind use of this nifty short cut.

<p>
<!-- !split -->

<h3 id="___sec9" class="anchor">Example: Taking the square root of a number </h3>

(Example 3.6.2 in Sivia)

<ul>
<li> Assume that the amplitude of a Bragg peak is measured with an uncertainty \( A = A_0 \pm \sigma_A \) from a least-squares fit to experimental data.</li>
<li> The Bragg peak amplitude is proportional to the square of a complex structure function: \( A = |F|^2 \equiv f^2 \).</li>
<li> What is \( f = f_0 \pm \sigma_f \)?</li>
</ul>

Obviously, we have that \( f_0 = \sqrt{A_0} \). Differentiate the relation, square and take the expectation value
$$
\langle \delta A^2 \rangle = 4 f_0^2 \langle \delta f^2 \rangle \quad 
\Leftrightarrow \quad 
\sigma_f = \frac{\sigma_A}{2 \sqrt{A_0}},
$$

where we have used the Gaussian approximation for the pdfs.

<p>
But what happens if the best fit gives \( A_0 < 0 \), which would not be impossible if we have weak and strongly overlapping peaks. The above equation obviously does not work since \( f_0 \) would be a complex number.

<p>
<!-- !split -->
We have made two mistakes:

<ol>
<li> Likelihood is not posterior!</li>
<li> The Gaussian approximation around the peak does not always work.</li>
</ol>

<!-- !split -->
Consider first the best fit of the signal peak. It implies that the likelihood can be approximated by
$$
p(D | A, I) \propto \exp \left[ -\frac{(A-A_0)^2}{2\sigma_A^2} \right].
$$

<p>
However, the posterior for \( A \) is \( p(A|D,I) \propto p(D|A,I) p(A|I) \) and we should use the fact that we know that \( A \ge 0 \).

<p>
<!-- !split -->
We will incorporate this information through a simple step-function prior
$$
p(A|I) = \left\{
\begin{array}{ll}
\frac{1}{A_\mathrm{max}}, & 0 \le A \le A_\mathrm{max}, \\
0, & \mathrm{otherwise}.
\end{array}
\right.
$$

This implies that the posterior will be a truncated Gaussian, and its maximum will always be above zero.

<p>
<!-- !split -->
This also implies that we cannot use the Gaussian approximation. Instead we will do the proper calculation using the transformation \eqref{eq:transformation}
$$
p(f|D,I) = p(A|D,I) \left| \frac{dA}{df} \right| = 2 f p(A|D,I)
$$

<p>
In the end we find the proper Bayesian error propagation given by the pdf
$$
p(f|D,I) \propto \left\{
\begin{array}{ll}
f \exp \left[ -\frac{(A-A_0)^2}{2\sigma_A^2} \right], & 0 \le f \le \sqrt{A_\mathrm{max}}, \\
0, & \mathrm{otherwise}.
\end{array}
\right.
$$

<p>
<!-- !split -->
Let us visualize the difference between the Bayesian and the naive error propagation for a few scenarios.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">A_posterior</span>(A,A0,sigA):
    pA <span style="color: #666666">=</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(A<span style="color: #666666">-</span>A0)<span style="color: #666666">**2/</span>(<span style="color: #666666">2*</span>sigA<span style="color: #666666">**2</span>))
    <span style="color: #008000; font-weight: bold">return</span> pA<span style="color: #666666">/</span>np<span style="color: #666666">.</span>max(pA)

<span style="color: #408080; font-style: italic"># Wrong analysis</span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">f_likelihood</span>(f,A0,sigA):
    sigf <span style="color: #666666">=</span> sigA <span style="color: #666666">/</span> (<span style="color: #666666">2*</span>np<span style="color: #666666">.</span>sqrt(A0))
    pf <span style="color: #666666">=</span> np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(f<span style="color: #666666">-</span>np<span style="color: #666666">.</span>sqrt(A0))<span style="color: #666666">**2/</span>(<span style="color: #666666">2*</span>sigf<span style="color: #666666">**2</span>))
    <span style="color: #008000; font-weight: bold">return</span> pf<span style="color: #666666">/</span>np<span style="color: #666666">.</span>max(pf)

<span style="color: #408080; font-style: italic"># Correct error propagation</span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">f_posterior</span>(f,A0,sigA):
    pf <span style="color: #666666">=</span> f<span style="color: #666666">*</span>np<span style="color: #666666">.</span>exp(<span style="color: #666666">-</span>(f<span style="color: #666666">**2-</span>A0)<span style="color: #666666">**2/</span>(<span style="color: #666666">2*</span>sigA<span style="color: #666666">**2</span>))
    <span style="color: #008000; font-weight: bold">return</span> pf<span style="color: #666666">/</span>np<span style="color: #666666">.</span>max(pf)
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">for</span> (A0,sigA) <span style="color: #AA22FF; font-weight: bold">in</span> [(<span style="color: #666666">9</span>,<span style="color: #666666">1</span>),(<span style="color: #666666">1</span>,<span style="color: #666666">9</span>),(<span style="color: #666666">-20</span>,<span style="color: #666666">9</span>)]:
    maxA <span style="color: #666666">=</span> <span style="color: #008000">max</span>(<span style="color: #666666">2*</span>A0,<span style="color: #666666">3*</span>sigA)
    A_arr <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">0.01</span>,maxA)
    f_arr <span style="color: #666666">=</span> np<span style="color: #666666">.</span>sqrt(A_arr)
    fig,ax<span style="color: #666666">=</span>plt<span style="color: #666666">.</span>subplots(<span style="color: #666666">1</span>,<span style="color: #666666">2</span>,figsize<span style="color: #666666">=</span>(<span style="color: #666666">10</span>,<span style="color: #666666">4</span>))
    ax[<span style="color: #666666">0</span>]<span style="color: #666666">.</span>plot(A_arr,A_posterior(A_arr,A0,sigA))
    ax[<span style="color: #666666">1</span>]<span style="color: #666666">.</span>plot(f_arr,f_posterior(f_arr,A0,sigA),label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Bayesian&#39;</span>)
    <span style="color: #008000; font-weight: bold">if</span> A0<span style="color: #666666">&gt;0</span>:
        ax[<span style="color: #666666">1</span>]<span style="color: #666666">.</span>plot(f_arr,f_likelihood(f_arr,A0,sigA),<span style="color: #BA2121">&#39;--&#39;</span>,label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Naive&#39;</span>)
    ax[<span style="color: #666666">0</span>]<span style="color: #666666">.</span>set(xlabel<span style="color: #666666">=</span><span style="color: #BA2121">&#39;A&#39;</span>,ylabel<span style="color: #666666">=</span><span style="color: #BA2121">&#39;p(A|D,I)&#39;</span>)
    plt<span style="color: #666666">.</span>text(<span style="color: #666666">0.55</span>,<span style="color: #666666">0.8</span>,f<span style="color: #BA2121">&#39;$A={A0}$, $\sigma_A={sigA}$&#39;</span>, transform<span style="color: #666666">=</span>ax[<span style="color: #666666">0</span>]<span style="color: #666666">.</span>transAxes,fontsize<span style="color: #666666">=16</span>)
    ax[<span style="color: #666666">1</span>]<span style="color: #666666">.</span>set(xlabel<span style="color: #666666">=</span><span style="color: #BA2121">&#39;f&#39;</span>,ylabel<span style="color: #666666">=</span><span style="color: #BA2121">&#39;p(f|D,I)&#39;</span>)
    ax[<span style="color: #666666">1</span>]<span style="color: #666666">.</span>legend(loc<span style="color: #666666">=</span><span style="color: #BA2121">&#39;best&#39;</span>)
    plt<span style="color: #666666">.</span>tight_layout()
</pre></div>
<p>
<!-- !split -->

<p>
<center>  <!-- FIGURE -->
<hr class="figure">
<center><p class="caption">Figure 2:  The left-hand panels show the posterior pdf for the amplitude of a Bragg peak in three different scenarios. The right-hand plots are the corresponding pdfs for the modulus of the structure factor \( f=\sqrt{A} \). The solid lines correspond to a full bayesian error propagation, while the dashed lines are obtained with the short-cut error propagation. </p></center>
<p><img src="fig/error_square_root_9_1.png" align="bottom" width=700></p>
</center>

<p>
<!-- !split -->
<br /><br /><center><p><img src="fig/error_square_root_1_9.png" align="bottom" width=700></p></center><br /><br />

<p>
<!-- !split -->
<br /><br /><center><p><img src="fig/error_square_root_-20_9.png" align="bottom" width=700></p></center><br /><br />

<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="http://..."><img width="250" align=right src="http://..."></a>
</footer>
-->


<center style="font-size:80%">
<!-- copyright --> &copy; 2018-2019, Christian Forss&#233;n. Released under CC Attribution-NonCommercial 4.0 license
</center>


</body>
</html>
    

