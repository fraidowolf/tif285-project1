TITLE: Learning from data: Bayesian Parameter Estimation
AUTHOR: Christian Forss√©n {copyright, 2018-present|CC BY-NC} at Department of Physics, Chalmers University of Technology, Sweden
DATE: today


!split
FIGURE:[fig/m1m2.png, width=400 frac=0.8] Joint pdf for the masses of two black holes merging obtained from the data analysis of a gravitational wave signal. This representation of a joint pdf is known as a corner plot. label{fig:gw}

!split
======= Inference With Parametric Models  =======
!bblock
Inductive inference with parametric models is a very important tool in the natural sciences.
* Consider $N$ different models $M_i$ ($i = 1, \ldots, N$), each with parameters $\boldsymbol{\theta}_i$. Each of them implies a sampling distribution for possible data

!bt
\[
p(D|\boldsymbol{\theta}_i, M_i)
\]
!et

* The likelihood function is the pdf of the actual, observed data ($D_\mathrm{obs}$) given a set of parameters $\boldsymbol{\theta}_i$:
!bt
\[
\mathcal{L}_i (\boldsymbol{\theta}_i) \equiv p(D_\mathrm{obs}|\boldsymbol{\theta}_i, M_i)
\]
!et
* We may be uncertain about $M_i$ (model uncertainty),
* or uncertain about $\boldsymbol{\theta}_i$ (parameter uncertainty).
!eblock

!split
!bblock
- Parameter Estimation: Premise = We have chosen a model (say $M_1$)<linebreak>
  $\Rightarrow$ What can we say about its parameters $\boldsymbol{\theta}_1$?
  
- Model comparison: Premise = We have a set of different models $\{M_i\}$<linebreak> 
  $\Rightarrow$ How do they compare with each other? Do we have evidence to say that, e.g. $M_1$, is better than the other models?
  
- Model adequacy: Premise = We have a model $M_1$<linebreak> 
  $\Rightarrow$ Is $M_1$ adequate?
  
- Hybrid Uncertainty: Models share some common params: $\boldsymbol{\theta}_i = \{ \boldsymbol{\varphi}, \boldsymbol{\eta}_i\}$<linebreak>
  $\Rightarrow$ What can we say about $\boldsymbol{\varphi}$? (Systematic error is an example)
!eblock

!split
===== Parameter estimation  =====
!bblock
Overview comments:
* In general terms, ``parameter estimation'' in physics means obtaining values for parameters (constants) that appear in a theoretical model which describes data (exceptions to this general definition exist of course).
* Conventionally this process is known as ``parameter fitting'' and the goal is to find the ``best fit''.
* We will make particular interpretations of these phrases from our Bayesian point of view.
* We will also see how familiar ideas like ``least-squares optimization'' show up from a Bayesian perspective.
!eblock


!split
===== Bayesian parameter estimation =====
!bblock
We will now consider the very important task of model parameter estimation using statistical inference. 

Let us first remind ourselves what can go wrong in a fit. We have encountered both _underfitting_ (model is not complex enough to describe the variability in the data) and _overfitting_ (model tunes to data fluctuations, or terms are underdetermined causing them playing off each other). Bayesian methods can prevent/identify both these situations.
!eblock

!split
#===== Example: Measured flux from a star (single parameter) =====
# #include "ExampleSinglePhotonCount.do.txt"

!split
======= Example: Gaussian noise and averages =======
The example in the demonstration notebook is from Sivia's book. How do we infer the mean and standard deviation of a Gaussian distribution from $M$ measurements $D \in \{ x_k \}_{k=0}^{M-1}$ that should be distributed according to a normal distribution $p( D | \mu,\sigma,I)$?

!split
Start from Bayes theorem
!bt
\[
p(\mu,\sigma | D, I) = \frac{p(D|\mu,\sigma,I) p(\mu,\sigma|I)}{p(D|I)}
\]
!et 
* Remind yourself about the names of the different terms.
* It should become intuitive what the different probabilities (pdfs) describe.
* Bayes theorem tells you how to flip from (hard-to-compute) $p(\mu,\sigma | D, I) \Leftrightarrow p(D|\mu,\sigma,I)$ (easier-to-compute).

!split
Aside on the denominator, which is known as the ``data probability'' or ``marginalized likelihood'' or ``evidence''. 
* With $\theta$ denoting a general vector of parameters we must have
!bt
\[
p(D|I) = \int d\theta p(D|\theta,I) p(\theta|I).
\]
!et
* This integration (or marginalization) over all parameters is often difficult to perform.
* Fortunately, for _parameter estimation_ we don't need $p(D|I)$ since it doesn't depend on $\theta$. We usually only need relative probabilities, or we can determine the normalization $N$ after we have computed the unnormalized posterior 
!bt
\[
p(\theta | D,I) = \frac{1}{N} p(D|\theta,I) p(\theta|I).
\]
!et

!split
If we use a uniform prior $p(\theta | I ) \propto 1$ (in a finite volume), then the posterior is proportional to the _likelihood_
!bt
\[
p(\theta | D,I) \propto p(D|\theta,I) = \mathcal{L}(\theta)
\]
!et
In this particular situation, the mode of the likelihood (which would correspond to the point estimate of maximum likelihood) is equivalent to the mode of the posterior pdf in the Bayesian analysis.

!split
The real use of the prior, however, is to include into the analysis any additional information that you might have. The prior statement makes such additional assumptions and information very explicit.

But how do we actually compute the posterior in practice. Most often we won't be able to get an analytical expression, but we can sample the distribution using a method known as Markov Chain Monte Carlo (MCMC).

!split
======= Example: Fitting a straight line =======
The next example that we will study is the well known fit of a straight line.

* Here the theoretical model is
!bt
\[
y_\mathrm{th}(x; \theta) = m x + b,
\]
!et
with parameters $\theta = [b,m]$.

* The statistical model for the data is
!bt
\[
y_{\mathrm{exp},i} = y_{\mathrm{th},i} + \delta y_{\mathrm{exp},i},
\]
!et
where we often assume that the experimental errors are independent and normally distributed so that
!bt
\[
y_i = \mathcal{N} \left( y_\mathrm{th}(x_i; \theta), e_i^2 \right).
\]
!et

* Is independent errors always a good approximation?
* An even better statistical model for theoretical models with a quantified, finite resolution would be
!bt
\[
y_\mathrm{exp} = y_\mathrm{th} + \delta y_\mathrm{exp} + \delta y_\mathrm{th}.
\]
!et

!split
=== Why normal distributions? ===
Let us give a quick motivation why Gaussian distributions show up so often. Say that we have a pdf $p(\theta | D,I)$. Our best estimate from this pdf will be $\theta_0$ where
!bt
\[ 
\left. 
\frac{ \partial p }{ \partial \theta }
\right|_{\theta_0} = 0, \qquad
\left. \frac{ \partial^2 p }{ \partial \theta^2 }
\right|_{\theta_0} < 0.
\]
!et
The distribution usually varies very rapidly so we study $L(\theta) \equiv \log p$ instead.
Near the peak, it behaves as
!bt
\[
L(\theta) = L(\theta_0) + \frac{1}{2} \left. \frac{\partial^2 L}{\partial \theta^2} \right|_{\theta_0} \left( \theta - \theta_0 \right)^2 + \ldots,
\]
!et
where the first-order term is zero since we are expanding around a maximum and $\partial L / \partial\theta = 0$.

!split
If we neglect higher-order terms we find that 
!bt
\[
p(\theta|D,I) \approx A \exp \left[ \frac{1}{2} \left. \frac{\partial^2 L}{\partial \theta^2} \right|_{\theta_0} \left( \theta - \theta_0 \right)^2  \right],
\]
!et
which is a Gaussian $\mathcal{N}(\mu,\sigma^2)$ with
!bt
\[
\mu = \theta_0, \qquad \sigma^2 = \left( - \left. \frac{\partial^2 L}{\partial \theta^2} \right|_{\theta_0} \right)^{-1/2}.
\]
!et

!split
===== Correlations =====
In the ``fitting a straight-line'' example you should find that the joint pdf for the slope and the intercept $[m, b]$ corresponds to a slanted ellipse. That result implies that the model parameters are _correlated_.

* Try to understand the correlation that you find in this example.

Let us explore correlations by studying the behavior of the pdf at the maximum.
A Taylor expansion for a bivariate pdf $p(x,y)$ around the mode $(x_0,y_0)$ gives
!bt
\[
p(x,y) \approx p(x_0,y_0) + \frac{1}{2} \begin{pmatrix} x-x_0 & y-y_0 \end{pmatrix}
H
\begin{pmatrix} x-x_0 \\ y-y_0 \end{pmatrix},
\]
!et
where $H$ is the symmetric Hessian matrix
!bt
\[
\begin{pmatrix}
A & C \\ C & B
\end{pmatrix}, 
\]
!et
with elements
!bt
\[
A = \left. \frac{\partial^2 p}{\partial x^2} \right|_{x_0,y_0}, \quad
B = \left. \frac{\partial^2 p}{\partial y^2} \right|_{x_0,y_0}, \quad
C = \left. \frac{\partial^2 p}{\partial x \partial y} \right|_{x_0,y_0}.
\]
!et


!split
* So in this quadratic approximation the contour is an ellipse centered at $(x_0,y_0)$ with orientation and eccentricity determined by $A,B,C$.
* The principal axes are found from the eigenvectors of $H$.
* Depending on the skewness of the ellipse, the parameters are either (i) not correlated, (ii) correlated, or (iii) anti-correlated.
* Take a minute to consider what that implies.
